{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM3r3vaWOff5xbek/A2KKHt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b05902062/TinyLlama/blob/main/sft/FunctionCallInteractive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slueteQfljye"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
        "\n",
        "model_name: str = \"ZihminWang/TinyLlama-1.1B-Chat-v1.0-user-intention-v0.1\"\n",
        "#model_name: str = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "print(f\"Loading tokenizer for {model_name}...\")\n",
        "# Load tokenizer, ensuring left-padding for decoder-only models for generation\n",
        "# And setting pad_token if not explicitly defined\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
        "if tokenizer.pad_token is None:\n",
        "    # Most modern LLMs should have an EOS token that can double as PAD\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # If adding a truly *new* token (e.g., tokenizer.add_special_tokens({'pad_token': '[PAD]'})),\n",
        "    # you would need to resize model embeddings: model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "print(f\"Loading model {model_name}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "print(f\"Model moved to {device}\")\n",
        "\n",
        "\n",
        "def generate_text_with_model(\n",
        "    prompt: str = \"Once upon a time,\",\n",
        "    max_new_tokens: int = 512,\n",
        "    do_sample: bool = True,\n",
        "    temperature: float = 0.9,\n",
        "    top_k: int = 50,\n",
        "    num_beams: int = 1, # Set to >1 for beam search\n",
        "    num_return_sequences: int = 1,\n",
        "    repetition_penalty: float = 1.0,\n",
        "    no_repeat_ngram_size: int = 0\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates text using a Hugging Face Transformers decoder-only model.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The starting text for generation.\n",
        "        max_new_tokens (int): The maximum number of new tokens to generate.\n",
        "        do_sample (bool): Whether to use sampling (True) or greedy/beam search (False).\n",
        "        temperature (float): Controls the randomness of sampling. Higher = more random. (Used with do_sample=True).\n",
        "        top_k (int): Filters out low probability tokens. (Used with do_sample=True).\n",
        "        num_beams (int): Number of beams for beam search. Set to > 1 for beam search.\n",
        "                         If num_beams > 1, do_sample must be False.\n",
        "        num_return_sequences (int): How many independent sequences to generate.\n",
        "        repetition_penalty (float): Penalizes repeated tokens/ngrams. >1.0 discourages repetition.\n",
        "        no_repeat_ngram_size (int): All ngrams of this size can only occur once.\n",
        "                                    Set to > 0 to prevent repeating phrases.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- Prepare the input ---\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    #print(f\"\\nGenerating text for prompt: '{prompt}'\")\n",
        "    #print(f\"Generation parameters: {{'max_new_tokens': {max_new_tokens}, 'do_sample': {do_sample}, 'temperature': {temperature}, 'top_k': {top_k}, 'num_beams': {num_beams}, 'num_return_sequences': {num_return_sequences}, 'repetition_penalty': {repetition_penalty}, 'no_repeat_ngram_size': {no_repeat_ngram_size}}}\")\n",
        "\n",
        "    # --- Define GenerationConfig (recommended for clarity and reusability) ---\n",
        "    # The GenerationConfig object centralizes generation parameters.\n",
        "    # It will use model.config defaults if not explicitly overridden.\n",
        "    generation_config = GenerationConfig(\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        do_sample=do_sample,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        eos_token_id=tokenizer.eos_token_id, # Always specify this\n",
        "        pad_token_id=tokenizer.pad_token_id, # Always specify this to avoid warnings\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        repetition_penalty=repetition_penalty,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size,\n",
        "        # Add other parameters like top_p, typical_p, diversity_penalty etc.\n",
        "        # based on your generation strategy\n",
        "    )\n",
        "\n",
        "    # --- Generate Text ---\n",
        "    # The .generate() method handles all the logic:\n",
        "    # - Auto-regressive decoding\n",
        "    # - Applying attention masks and padding (if batching)\n",
        "    # - Implementing different decoding strategies (greedy, beam, sampling)\n",
        "    # - Stopping criteria (max_new_tokens, eos_token_id)\n",
        "    generated_ids = model.generate(\n",
        "        input_ids,\n",
        "        generation_config=generation_config, # Pass the config object\n",
        "        # You can also pass individual parameters directly if not using GenerationConfig:\n",
        "        # max_new_tokens=max_new_tokens,\n",
        "        # do_sample=do_sample,\n",
        "        # temperature=temperature,\n",
        "        # eos_token_id=tokenizer.eos_token_id,\n",
        "        # pad_token_id=tokenizer.pad_token_id,\n",
        "        # ...\n",
        "    )\n",
        "\n",
        "    # --- Decode and Print Results ---\n",
        "    decoded_texts = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    #print(\"\\n--- Generated Texts ---\")\n",
        "    for i, text in enumerate(decoded_texts):\n",
        "        print(f\"Prompt: \")\n",
        "        print(text)\n",
        "        print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_format = (\n",
        "      \"You have access to the following function. \"\n",
        "      \"{system} \"\n",
        "      \"Respond to the user AS USUAL if invoking the function won't help the task. \"\n",
        "      \"If you decide to invoke the function, you MUST put it in the format of \"\n",
        "      '''{{\"function\": {{\"<name_of_function_to_use>\": {{\"intent\": \"true\"}}}}}} '''\n",
        "      \"You SHOULD NOT include any other text in the response if you intent to invoke the function. \"\n",
        "\n",
        "      \"### Instruction:\\n{user}\\n\\n### Response: \"\n",
        ")\n",
        "\n",
        "example = {\n",
        "    \"system\" : '''{\n",
        "      \"name\": \"generate_image\",\n",
        "      \"description\": \"Generate an image from a text description (prompt).\",\n",
        "      \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"prompt\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"A detailed text description of the image to generate. Be specific about subjects, styles, colors, and lighting.\"\n",
        "          },\n",
        "          \"negative_prompt\": {\n",
        "            \"type\": \"string\",\n",
        "            \"description\": \"A text description of things to exclude from the image. For example, 'blurry, distorted, ugly'.\"\n",
        "          },\n",
        "          \"width\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"The desired width of the generated image in pixels. Common values are 512, 768, 1024.\",\n",
        "            \"minimum\": 64,\n",
        "            \"maximum\": 2048\n",
        "          },\n",
        "          \"height\": {\n",
        "            \"type\": \"integer\",\n",
        "            \"description\": \"The desired height of the generated image in pixels. Common values are 512, 768, 1024.\",\n",
        "            \"minimum\": 64,\n",
        "            \"maximum\": 2048\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    } ''',\n",
        "    \"user\" : \"Give me a image with a boy and a girl playing on a slide. width and height are both 720px.\",\n",
        "}\n",
        "\n",
        "generate_text_with_model(prompt=prompt_format.format(**example))"
      ],
      "metadata": {
        "id": "M4DBMRlnomUD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}