{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyMP+rpSumN3aBmvY7EwVDe1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/b05902062/TinyLlama/blob/main/sft/FunctionCallingFinetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I575Fw53tRtA"
      },
      "outputs": [],
      "source": [
        "# Clone finetuning code.\n",
        "\n",
        "!git clone https://github.com/b05902062/TinyLlama.git\n",
        "%cd ./TinyLlama/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upgrade some modules to fix finetuning errors.\n",
        "\n",
        "!pip install evaluate\n",
        "!pip install --upgrade datasets fsspec"
      ],
      "metadata": {
        "id": "nHt1C4721z_f",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune for user intention from TinyLlama/TinyLlama-1.1B-Chat-v1.0 checkpoint for 1 epoch.\n",
        "\n",
        "!python /content/TinyLlama/sft/finetune.py \\\n",
        "    --model_name_or_path TinyLlama/TinyLlama-1.1B-Chat-v1.0 \\\n",
        "    --output_dir ./output/0001_run_instruction_finetune \\\n",
        "    --logging_strategy steps \\\n",
        "    --logging_steps 200 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --eval_strategy steps \\\n",
        "    --eval_steps 200 \\\n",
        "    --eval_dataset_size 512 \\\n",
        "    --max_eval_samples 512 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --save_strategy steps \\\n",
        "    --save_steps 1000 \\\n",
        "    --data_seed 42 \\\n",
        "    --save_total_limit 15 \\\n",
        "    --max_new_tokens 512 \\\n",
        "    --dataloader_num_workers 3 \\\n",
        "    --group_by_length=True \\\n",
        "    --remove_unused_columns False \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --dataset ZihminWang/user-intention \\\n",
        "    --dataset_format ZihminWang/user-intention \\\n",
        "    --source_max_len 1024 \\\n",
        "    --target_max_len 1024 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --adam_beta2 0.999 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --seed 0 \\\n",
        "    --trust_remote_code \\\n",
        "    --report_to tensorboard \\\n",
        "    --load_checkpoint True"
      ],
      "metadata": {
        "id": "X8OXgeIsuHYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluta models on test split. Automatically use your checkpoint if you have run the above cell. If not, pull from ZihminWang/TinyLlama-1.1B-Chat-v1.0-user-intention-v0.1.\n",
        "\n",
        "!python /content/TinyLlama/sft/finetune.py \\\n",
        "    --model_name_or_path ZihminWang/TinyLlama-1.1B-Chat-v1.0-user-intention-v0.1 \\\n",
        "    --output_dir ./output/0001_run_instruction_finetune \\\n",
        "    --logging_strategy no \\\n",
        "    --logging_steps 1 \\\n",
        "    --save_strategy no \\\n",
        "    --save_steps 1 \\\n",
        "    --eval_strategy no \\\n",
        "    --eval_steps 1000 \\\n",
        "    --eval_dataset_size 512 \\\n",
        "    --max_eval_samples 512 \\\n",
        "    --per_device_eval_batch_size 16 \\\n",
        "    --data_seed 42 \\\n",
        "    --dataloader_num_workers 3 \\\n",
        "    --group_by_length=False \\\n",
        "    --remove_unused_columns False \\\n",
        "    --do_train False \\\n",
        "    --do_eval False \\\n",
        "    --do_predict \\\n",
        "    --warmup_ratio 0.05 \\\n",
        "    --lr_scheduler_type cosine \\\n",
        "    --dataset ZihminWang/user-intention \\\n",
        "    --dataset_format ZihminWang/user-intention \\\n",
        "    --source_max_len 1024 \\\n",
        "    --target_max_len 1024 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --learning_rate 1e-5 \\\n",
        "    --adam_beta2 0.999 \\\n",
        "    --max_grad_norm 1.0 \\\n",
        "    --weight_decay 0.01 \\\n",
        "    --seed 0 \\\n",
        "    --trust_remote_code \\\n",
        "    --report_to tensorboard \\\n",
        "    --load_checkpoint True \\\n",
        "    --predict_with_generate True \\\n",
        "    --max_predict_samples 512 \\"
      ],
      "metadata": {
        "id": "l8dtPBAr2Doh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show training progress.\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir output/0001_run_instruction_finetune/runs"
      ],
      "metadata": {
        "id": "9IeAuYdQYUu-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "hHF2JMorTaej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Repo to upload your lastest checkpoint and training progress to.\n",
        "\n",
        "new_repo_name = input(\"Please enter the new Hugging Face repository name (e.g., 'your-username/your-repo'): \").strip()"
      ],
      "metadata": {
        "id": "l3KtaW2JQm2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload your lastest checkpoint and training progress.\n",
        "\n",
        "from huggingface_hub import upload_folder\n",
        "import os\n",
        "import glob # For pattern matching file paths\n",
        "\n",
        "# Define the base output directory where your checkpoints are saved\n",
        "base_output_dir = \"/content/TinyLlama/output/0001_run_instruction_finetune\"\n",
        "\n",
        "# Construct the pattern to find all checkpoint folders\n",
        "# The `checkpoint-*` pattern will match folders like 'checkpoint-100', 'checkpoint-200', etc.\n",
        "checkpoint_pattern = os.path.join(base_output_dir, \"checkpoint-*\")\n",
        "\n",
        "# Use glob to find all matching checkpoint directories\n",
        "# glob.glob returns a list of paths\n",
        "all_checkpoints = glob.glob(checkpoint_pattern)\n",
        "\n",
        "# Sort the checkpoints to find the latest one.\n",
        "# Checkpoint folders are usually named 'checkpoint-STEP_NUMBER'.\n",
        "# Sorting alphabetically/numerically will naturally put the highest step number last.\n",
        "if all_checkpoints:\n",
        "    for checkpoint_folder in all_checkpoints:\n",
        "        training_args_path = os.path.join(checkpoint_folder, \"training_args.bin\")\n",
        "        if os.path.exists(training_args_path):\n",
        "            os.remove(training_args_path)\n",
        "    print(\"\\nClean-up complete. You can now upload your checkpoints.\")\n",
        "\n",
        "    all_checkpoints.sort(key=lambda x: int(os.path.basename(x).split('-')[1]))\n",
        "    latest_checkpoint_folder = all_checkpoints[-1] # The last element after sorting is the latest\n",
        "    print(f\"Found latest checkpoint folder: {latest_checkpoint_folder}\")\n",
        "\n",
        "    upload_folder(\n",
        "      folder_path=latest_checkpoint_folder,\n",
        "      repo_id=new_repo_name,\n",
        "      commit_message=\"Upload latest fine-tuned TinyLlama checkpoint\",\n",
        "      repo_type=\"model\", # or \"dataset\", \"space\"\n",
        "    )\n",
        "    print(f\"Model uploaded to https://huggingface.co/{new_repo_name}\")\n",
        "else:\n",
        "    print(f\"No checkpoint folders found in: {base_output_dir}\")\n",
        "\n",
        "\n",
        "# 3. Define the path to your local TensorBoard logs\n",
        "# This should be the directory containing your 'events.out.tfevents.*' files\n",
        "tensorboard_log_dir = \"/content/TinyLlama/output/0001_run_instruction_finetune/runs\"\n",
        "\n",
        "# 4. Upload the logs\n",
        "# The `path_in_repo` argument specifies where the files will be placed in your HF repo.\n",
        "# It's good practice to mirror your local structure or put them in a 'runs' folder.\n",
        "upload_folder(\n",
        "    folder_path=tensorboard_log_dir,\n",
        "    repo_id=new_repo_name,\n",
        "    commit_message=\"Add TensorBoard training logs\",\n",
        "    repo_type=\"model\",\n",
        "    path_in_repo=\"runs\", # This will put the logs inside a 'runs' folder in your HF repo\n",
        ")\n",
        "\n",
        "print(f\"TensorBoard logs uploaded to https://huggingface.co/{new_repo_name}/tree/main/runs\")\n",
        "print(f\"You should see a 'Training metrics' tab on your model page: https://huggingface.co/{new_repo_name}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z3Mpb3QNj916"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}